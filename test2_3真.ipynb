{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7diJYEIX66pd",
        "outputId": "91ca57b9-9bde-4fe4-db9a-26cf085da404"
      },
      "outputs": [],
      "source": [
        "pip install gym==0.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "d2bQJcc960Ma",
        "outputId": "bce44eb0-0b38-4983-b587-7245997eecd0"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "from sre_constants import SUCCESS\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from matplotlib import pylab as plt\n",
        "import gym\n",
        "#from gym import wrappers \n",
        "from IPython.display import clear_output\n",
        "\n",
        "L1 = 2\n",
        "L2 = 70\n",
        "L3 = 40\n",
        "L4 = 3\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(L1, L2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(L2, L3),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(L3,L4)\n",
        ")\n",
        "\n",
        "model2 = copy.deepcopy(model) #target network\n",
        "model2.load_state_dict(model.state_dict()) #將原始Q網路中的參數複製給目標網路\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "gamma = 0.9\n",
        "epsilon = 0.6\n",
        "\n",
        "env = gym.envs.make('MountainCar-v0')\n",
        "env = env.unwrapped\n",
        "#env = wrappers.Monitor(env,\"C:\\testml\\video\",video_callable=lambda count:count%10==0)\n",
        "#env.Monitor.start('C:\\testml\\video',video_callable=lambda count: count % 10 == 0)\n",
        "\n",
        "from collections import deque\n",
        "epochs = 1000 #訓練次數\n",
        "losses = []\n",
        "step_num = []\n",
        "mem_size = 1000 \n",
        "batch_size = 64 #batch_size\n",
        "replay = deque(maxlen=mem_size) #replay buffer\n",
        "max_moves = 1000 #每場遊戲最多可以走幾步\n",
        "sync_freq = 200\n",
        "success_flag=0\n",
        "success_num=0\n",
        "j=0\n",
        "for i in range(epochs):\n",
        "    state1_ = env.reset()\n",
        "    state1 = torch.from_numpy(state1_).float()\n",
        "    status = 1\n",
        "    success_flag=0\n",
        "    mov = 0     #記錄移動的步數\n",
        "    while(status == 1): \n",
        "        j += 1\n",
        "        mov += 1\n",
        "        qval = model(state1) #輸出各動作的Q值\n",
        "        #print(qval,i)\n",
        "        qval_ = qval.data.numpy()\n",
        "        if (random.random() < epsilon):\n",
        "            action_ = np.random.randint(0,2)\n",
        "        else:\n",
        "            action_ = np.argmax(qval_)\n",
        "        state2_, reward , done , info= env.step(action_)\n",
        "        #env.render() \n",
        "        #print(state2_)\n",
        "        \"\"\"\n",
        "        if reward != -1:\n",
        "            #step_num.append(mov)\n",
        "            success_flag=1\n",
        "            success_num+=1\n",
        "            reward=200\n",
        "        \"\"\"\n",
        "\n",
        "        if mov == 700 :\n",
        "            reward = reward -0.5\n",
        "        if state2_[0] >= -0.4 :\n",
        "            reward +=  0.5 + state2_[0]\n",
        "        if state2_[1] >= 0.02 :\n",
        "            reward +=   0.2 + state2_[1]\n",
        "\n",
        "        if state2_[0] > 0.5 and success_flag == 0:\n",
        "            status = 0\n",
        "            #reward += 20\n",
        "            success_num+=1\n",
        "            step_num.append(mov)\n",
        "            success_flag=1\n",
        "        \n",
        "        #if state2_[0] > 0.5:\n",
        "                #reward = 5\n",
        "        #elif state2_[0] <= -0.6:\n",
        "            #reward = -1 + state2_[0]\n",
        "        state2 = torch.from_numpy(state2_).float()\n",
        "        #print(i)\n",
        "        exp = (state1, action_, reward, state2, done)\n",
        "        replay.append(exp)\n",
        "        state1 = state2 \n",
        "        #print(state1)\n",
        "        #print(len(replay),batch_size)     \n",
        "\n",
        "        if len(replay) > batch_size:\n",
        "            minibatch = random.sample(replay, batch_size)\n",
        "            state1_batch_ = torch.cat([s1 for (s1,a,r,s2,d) in minibatch])\n",
        "            action_batch = torch.Tensor([a for (s1,a,r,s2,d) in minibatch])\n",
        "            reward_batch = torch.Tensor([r for (s1,a,r,s2,d) in minibatch])\n",
        "            state2_batch_ = torch.cat([s2 for (s1,a,r,s2,d) in minibatch])\n",
        "            done_batch = torch.Tensor([d for (s1,a,r,s2,d) in minibatch])\n",
        "            state1_batch = torch.reshape(state1_batch_, (64, 2))\n",
        "            state2_batch = torch.reshape(state2_batch_, (64, 2))\n",
        "\n",
        "            Q1 = model(state1_batch) \n",
        "            with torch.no_grad():     \n",
        "                Q2 = model2(state2_batch) \n",
        "            Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0])\n",
        "            X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n",
        "            loss = loss_fn(X, Y.detach())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "            losses.append(loss.item())    \n",
        "\n",
        "            if j % sync_freq == 0:\n",
        "                model2.load_state_dict(model.state_dict())\n",
        "            \n",
        "        if reward == 200 or mov > max_moves:\n",
        "            status = 0 \n",
        "            mov = 0\n",
        "        #losses.append(loss.item())  \n",
        "    if epsilon > 0.1: \n",
        "        epsilon -= (1/epochs) \n",
        "    \"\"\"if i % 100 == 0:\n",
        "        if i!=0 and i%1000 == 0 :\n",
        "            print(i,loss)\n",
        "        else:\n",
        "            print(i)\"\"\"\n",
        "    if success_flag == 0:\n",
        "        step_num.append(1000)\n",
        "    #print('\\r')\n",
        "    print(i, loss.item(),success_num)\n",
        "    #print('\\r\\b')\n",
        "#env.close()\n",
        "#test_model(model)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Steps\",fontsize=11)\n",
        "plt.ylabel(\"Loss\",fontsize=11)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(step_num)\n",
        "plt.xlabel(\"epochs\",fontsize=11)\n",
        "plt.ylabel(\"action number\",fontsize=11)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoOCAsojBpDh"
      },
      "outputs": [],
      "source": [
        "\"\"\"def test_model(model):\n",
        "  i = 0\n",
        "  env = gym.envs.make('MountainCar-v0')\n",
        "  env = env.unwrapped\n",
        "  for i in range(10):\n",
        "    state3_ = env.reset()  \n",
        "    env.render()\n",
        "    state3 = torch.from_numpy(state3_).float()\n",
        "    status = 1\n",
        "    mov = 0\n",
        "    while(status == 1):    \n",
        "      mov += 1\n",
        "      qval = model(state3)\n",
        "      qval_ = qval.data.numpy()\n",
        "      action_ = np.argmax(qval_) \n",
        "      print(action_)\n",
        "      observation, reward, done, info = env.step(action_)\n",
        "      state3=torch.from_numpy(observation).float()\n",
        "      #print(action_,reward,done)\n",
        "      #print(observation)\n",
        "      env.render()\n",
        "      if mov >= 1000:\n",
        "        status=0\n",
        "      #time.sleep(0.02)\n",
        "      #env.render()    \n",
        "    print(i)\n",
        "  env.close()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#env = gym.envs.make('MountainCar-v0')\n",
        "#env = env.unwrapped\n",
        "test_s=0\n",
        "for i in range(20):\n",
        "    state3_ = env.reset()  \n",
        "    env.render()\n",
        "    state3 = torch.from_numpy(state3_).float()\n",
        "    status = 1\n",
        "    mov = 0 \n",
        "    while(status == 1):    \n",
        "        mov += 1\n",
        "        qval = model(state3)\n",
        "        qval_ = qval.data.numpy()\n",
        "        action_ = np.argmax(qval_) \n",
        "        #print(action_)\n",
        "        observation, reward, done, info = env.step(action_)\n",
        "        state3=torch.from_numpy(observation).float()\n",
        "        #print(action_,reward,done)\n",
        "        #print(observation)\n",
        "        env.render()\n",
        "        if observation[0] > 0.5 :\n",
        "            status = 0\n",
        "            test_s += 1\n",
        "        if mov >= 1000:\n",
        "            status=0\n",
        "        \n",
        "        #env.render()    \n",
        "    print(i)\n",
        "print(test_s)\n",
        "#env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8A8lmbFnH77l",
        "outputId": "9c99e2bb-b14c-409b-bf40-d4d5a547ec4d"
      },
      "outputs": [],
      "source": [
        "#test_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd8bcd87f8f83f942a7c4d78815ec77e8b86279f8328141a8deb44e263b3b175"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
